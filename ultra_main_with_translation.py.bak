#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ultra_main.py — diffusers UNet2DModel + 견고한 마스크 매칭 + 마스크 어닐링 + 사후 정제(저주파/히스토그램/TV/경계)
- 마스크 의미: 1=Known, 0=Unknown(블라인드존) [기본적으로 반전하지 않음; 필요 시 --invert-mask]
- 초기화: Unknown(0)만 그레이스케일 가우시안, Known은 원본 유지
- DDRM 샘플링: 선형 관측자(H)로 Known 영역을 매 스텝 일치(원본 유지 보장)
- 마스크 어닐링: dilate_px>0이면 1차(팽창 마스크)→2차(원래 마스크) 2-패스 수행
- 사후 정제: Unknown만 대상으로 저주파/히스토그램/TV/경계 링 정규화로 미세 톤/시임 품질 향상
- 저장: k=0(초기화 프레임) 스킵, 이후 step_interval마다 저장(옵션). 기본은 마지막 결과만 저장
- Optuna 통합: orig와 유사(MAE@Unknown 최소)하도록 하이퍼파라미터 탐색 가능(--optuna)

CLI 예시:
python ultra_main.py \
  --images-dir /path/images --masks-dir /path/masks --output-dir /path/out --ckpt-path /path/ckpt.pt \
  --timesteps-list 80,200 --mask-dilate-px 2 --do-refine \
  --refine-iters 100 --w-lfreq 0.6 --w-hist 0.2 --w-tv 0.0015 --w-seam 0.9 \
  --optuna --optuna-trials 20 --optuna-samples 4
"""

import os
import re
import glob
from dataclasses import dataclass, asdict, replace
from pathlib import Path
from typing import List, Tuple, Optional, Dict

import argparse
import numpy as np
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import torchvision.utils as tvu

# ===== Optuna (optional) =====
try:
    import optuna
except ImportError:
    optuna = None

# ===== DDRM 샘플러 =====
from functions.denoising import efficient_generalized_steps, ddnm_steps

# ===== diffusers =====

from diffusers import UNet2DModel, DDPMScheduler

# ===== Inpainting operator =====
from functions.svd_replacement import Inpainting


# =========================
# 설정 데이터클래스
# =========================
@dataclass
class InpaintConfig:
    # 데이터 경로
    images_dir: str = "/home/juneyonglee/Desktop/ddrm/data/inpaint_inputs"
    masks_dir: str = "/home/juneyonglee/Desktop/ddrm/inp_masks/cy_mask/thresh_60_alpha2_0"
    output_dir: str = "/home/juneyonglee/Desktop/ddrm/outputs_ultrasound_ddrm"
    
    # CY→CN 번역 모드용 추가 경로
    translation_mode: bool = False  # True일 때 CY→CN 번역 모드 사용
    cn_targets_dir: str = "/home/juneyonglee/Desktop/ddrm/datasets/test_CN"  # CN 타겟 이미지 디렉토리

    # 모델
    ckpt_path: str = "/home/juneyonglee/Desktop/ddrm/final_mixed_training_model.pt"
    image_size: int = 512
    data_channels: int = 1
    model_in_channels: int = 3       # 학습 채널에 맞추기
    model_out_channels: int = 3

    # diffusers UNet2DModel 하이퍼파라미터(학습 때와 동일)
    layers_per_block: int = 2
    block_out_channels: tuple = (128, 256, 512, 512, 1024)
    down_block_types: tuple = ("DownBlock2D", "DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D")
    up_block_types: tuple = ("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D")
    attention_head_dim: int = 8

    # 샘플링/노이즈
    timesteps: int = 250
    eta_a: float = 0.0
    eta_b: float = 0.0
    eta_c: float = 0.0
    sigma0: float = 0.0  # 관측 y0에 추가할 측정노이즈 표준편차(선택)

    # 로더
    num_workers: int = 8
    seed: int = 42

    # 마스크 매칭/이진화
    mask_suffix: str = "_noaug.jpg"
    mask_bin_thresh: float = 0.5
    known_is_white: bool = True
    invert_mask: bool = True       # 기본: 반전하지 않음(최종 1=Known, 0=Unknown 유지)
    skip_missing: bool = True       # 마스크 없으면 샘플 스킵

    # 저장 옵션
    save_y0: bool = True
    save_orig: bool = True
    save_last_only: bool = True
    step_interval: int = 100

    # 베타 스케줄(diffusers 학습과 일치)
    num_diffusion_timesteps: int = 1000
    beta_schedule: str = "scaled_linear"
    prediction_type: str = "epsilon"

    # 초기화 제어 — Unknown(마스크 영역=0)만 그레이스케일 가우시안 노이즈
    masked_noise_init: bool = True
    init_noise_std: float = 1.0      # Unknown 영역에 넣을 초기 노이즈 표준편차

    # 어닐링/패스 제어
    mask_dilate_px: int = 8          # >0이면 1차 패스에서 Unknown 팽창
    do_refine: bool = True           # 사후 정제 수행 여부

    # 정제 가중치/횟수
    refine_iters: int = 150
    refine_lr: float = 0.1
    w_lfreq: float = 1.0             # 저주파 일치
    w_hist: float = 0.25             # 히스토그램(mean/std) 일치
    w_tv: float = 0.005              # TV 정규화
    w_seam: float = 0.5              # 경계 링 일치
    seam_width: int = 3              # 경계 링 너비(픽셀)

    # timesteps 리스트(여러 설정 반복)
    _timesteps_list: Tuple[int, ...] = (80, 200)

    # Optuna
    optuna_mode: bool = False
    optuna_trials: int = 20
    optuna_samples: int = 4  # 앞에서부터 몇 장으로 평가할지
    optuna_batch_size: int = 6  # Optuna 평가 시 배치 크기 (24GB VRAM용)
    optuna_n_jobs: int = 2  # Optuna 병렬 작업 수
    optuna_aggressive_memory: bool = False  # 더 공격적인 메모리 사용 (24GB+ VRAM용)

    # DDNM
    use_ddnm: bool = True

    # Patch-based inference
    use_patch_inference: bool = False
    patch_size: int = 256
    patch_stride: int = 128


# =========================
# 유틸
# =========================
def set_seed(seed: int):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def get_gpu_memory_info():
    """GPU 메모리 사용량 정보 반환"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3  # GB
        return {
            'allocated': allocated,
            'reserved': reserved,
            'max_allocated': max_allocated
        }
    return None

def to_tensor(img_pil: Image.Image) -> torch.Tensor:
    arr = np.asarray(img_pil, dtype=np.float32) / 255.0
    if arr.ndim == 2:
        arr = arr[None, ...]  # 1xHxW
    else:
        arr = arr.transpose(2, 0, 1)  # CxHxW
    return torch.from_numpy(arr)

def data_transform(x: torch.Tensor) -> torch.Tensor:
    return x * 2.0 - 1.0  # [0,1] -> [-1,1]

def inverse_data_transform(x: torch.Tensor) -> torch.Tensor:
    return (x + 1.0) / 2.0  # [-1,1] -> [0,1]

def load_grayscale(path: Path, size: int) -> Image.Image:
    img = Image.open(path).convert("L")
    if img.size != (size, size):
        img = img.resize((size, size), Image.BICUBIC)
    return img

def load_mask_array(path: Path, size: int, bin_thresh: float, known_is_white: bool) -> np.ndarray:
    m = Image.open(path).convert("L")
    if m.size != (size, size):
        m = m.resize((size, size), Image.NEAREST)
    m = np.asarray(m, dtype=np.float32) / 255.0
    m = (m >= bin_thresh).astype(np.float32)
    if not known_is_white:
        m = 1.0 - m
    return m  # HxW, 1=Known

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def contains_token(name: str, token: str, seps: str = "_+-. ") -> bool:
    start = 0
    n, t = len(name), len(token)
    while True:
        i = name.find(token, start)
        if i == -1:
            return False
        left_ok = (i == 0) or (name[i - 1] in seps)
        right_ok = (i + t == n) or (name[i + t] in seps)
        if left_ok and right_ok:
            return True
        start = i + 1

def extract_run_id(text: str) -> Optional[str]:
    m = re.search(r"V\d+_\d{3}", text)
    return m.group(0) if m else None

def tokens(s: str) -> set:
    return set([t for t in re.split(r"[+\-_. ]+", s) if t])

def mask_to_missing_indices(mask01: torch.Tensor, channels: int, device: torch.device) -> torch.Tensor:
    """
    mask01: HxW, 값 1=Known, 0=Unknown(복원 대상)
    반환: Unknown 위치들의 (채널 포함) 플랫 인덱스 (C*H*W 기준)
    """
    if mask01.dim() != 2:
        raise ValueError("mask01 must be 2D (HxW)")
    H, W = mask01.shape
    hw_flat = (mask01.reshape(-1) < 0.5).nonzero(as_tuple=False).squeeze(1).long()
    if channels == 1:
        idx = hw_flat
    else:
        stride = H * W
        idx = torch.cat([hw_flat + c * stride for c in range(channels)], dim=0)
    return idx.to(device)


def stem_equal(a: str, b: str) -> bool:
    return a.lower() == b.lower()

def first_token(s: str) -> str:
    return re.split(r"[+\-_. ]+", s, maxsplit=1)[0]

def contains_stem_boundary(mask_stem: str, img_stem: str) -> bool:
    return contains_token(mask_stem, img_stem)

def choose_mask_for_image_stem(img_stem: str, mask_files: List[Path]) -> Optional[Path]:
    if not mask_files:
        return None

    # 1) V패턴 우선
    img_v_pattern = re.search(r'V\d+_\d{3}', img_stem)
    if img_v_pattern:
        v_match = img_v_pattern.group(0)
        v_matches = [m for m in mask_files if v_match in m.stem]
        if v_matches:
            return min(v_matches, key=lambda p: len(p.stem))

    # 2) 정확 일치
    exact_matches = [m for m in mask_files if stem_equal(m.stem, img_stem)]
    if exact_matches:
        return exact_matches[0]

    # 3) 치환 규칙
    search_pattern = img_stem
    if 'CY_' in search_pattern:
        search_pattern = search_pattern.replace('CY_', 'CN_')
    if search_pattern.endswith('_101'):
        search_pattern = search_pattern[:-4] + '_001'
    elif search_pattern.endswith('_100'):
        search_pattern = search_pattern[:-4] + '_000'

    patterns_to_try = [
        search_pattern,
        search_pattern.replace('_PC_', '_PL_'),
        search_pattern.replace('_PL_', '_PC_')
    ]
    for pattern in patterns_to_try:
        matches = [m for m in mask_files if pattern in m.stem]
        if matches:
            return min(matches, key=lambda p: len(p.stem))

    # 4) 토큰 교집합
    img_tokens = tokens(img_stem)
    best_mask, best_score = None, 0
    for mask_file in mask_files:
        mask_tokens = tokens(mask_file.stem)
        common_tokens = img_tokens & mask_tokens
        score = len(common_tokens)
        if score > best_score:
            best_score, best_mask = score, mask_file
    return best_mask if best_score > 0 else None


# =========================
# 노이즈/마스크 유틸
# =========================
def create_grayscale_noise(shape: tuple, device: torch.device, std: float = 1.0, dtype: torch.dtype = None) -> torch.Tensor:
    """
    그레이스케일(채널 공유) 가우시안 노이즈 생성.
    - 입력 shape가 (B,C,H,W)일 때만 채널 공유. 그 외(shape가 2D 등)는 표준 가우시안 동일 shape로 반환.
    - dtype/device 를 호출측(대상 텐서)과 일치시켜 연산 디바이스 혼선을 방지.
    """
    if dtype is None:
        dtype = torch.float32
    if len(shape) == 4:
        B, C, H, W = shape
        noise_1ch = torch.randn(B, 1, H, W, device=device, dtype=dtype) * std
        return noise_1ch.repeat(1, C, 1, 1)
    else:
        return torch.randn(*shape, device=device, dtype=dtype) * std


def _build_mask_stacks(mask01: torch.Tensor, C: int) -> Tuple[torch.Tensor, torch.Tensor]:
    if mask01.dim() != 2:
        raise ValueError("mask01 must be 2D (HxW)")
    known01 = mask01[None, None, :, :]  # 1x1xHxW
    known01_c = known01.repeat(1, C, 1, 1) if C > 1 else known01
    unknown01_c = 1.0 - known01_c
    return known01_c, unknown01_c

def dilate_unknown_mask(mask01: torch.Tensor, px: int) -> torch.Tensor:
    """
    mask01: 1=Known, 0=Unknown
    px 픽셀만큼 Unknown(0)을 팽창 → Known을 그만큼 깎음
    """
    if px <= 0:
        return mask01
    unknown = 1.0 - mask01
    k = 2 * px + 1
    unk = F.max_pool2d(unknown[None, None], kernel_size=k, stride=1, padding=px)
    unk = unk[0, 0]
    return 1.0 - torch.clamp(unk, 0, 1)

def make_seam_ring(mask01: torch.Tensor, width: int) -> torch.Tensor:
    """
    Known/Unknown 경계를 중심으로 너비 `width` 픽셀의 링 마스크(Unknown 쪽)에 1을 둠.
    """
    width = max(1, int(width))
    # 에지: |∇mask|
    k = 2 * width + 1
    m = mask01[None, None]
    maxed = F.max_pool2d(m, kernel_size=k, stride=1, padding=width)
    mined = -F.max_pool2d(-m, kernel_size=k, stride=1, padding=width)
    edge_band = (maxed - mined)[0, 0]  # [0,1] 범위 근방
    unknown = (mask01 < 0.5).float()
    ring = (edge_band > 0.0).float() * unknown
    return ring


# =========================
# 데이터셋
# =========================
class CYtoCNTranslationDataset(data.Dataset):
    """CY 이미지를 CN 이미지로 변환하는 목표의 데이터셋"""
    IMG_EXTS = {".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"}

    def __init__(self, cy_images_dir: str, cn_targets_dir: str, masks_dir: str, image_size: int,
                 data_channels: int, model_channels: int, mask_suffix: str,
                 mask_bin_thresh: float, known_is_white: bool, invert_mask: bool,
                 skip_missing: bool = True, log_unmatched_to: Optional[Path] = None):
        self.image_size = image_size
        self.data_channels = data_channels
        self.model_channels = model_channels
        self.mask_bin_thresh = mask_bin_thresh
        self.known_is_white = known_is_white
        self.invert_mask = invert_mask
        self.mask_suffix = mask_suffix
        self.skip_missing = skip_missing

        cy_dir = Path(cy_images_dir)
        cn_dir = Path(cn_targets_dir)
        mask_root = Path(masks_dir)

        # CY 이미지 파일들 수집
        cy_imgs: List[Path] = []
        for p in sorted(cy_dir.iterdir()):
            if p.is_file() and p.suffix.lower() in self.IMG_EXTS:
                cy_imgs.append(p)
        if not cy_imgs:
            raise RuntimeError(f"No CY images found in {cy_dir}")

        # 마스크 파일들 수집
        mask_files = [Path(x) for x in glob.glob(str(mask_root / "**" / f"*{mask_suffix}"), recursive=True)]
        if not mask_files:
            raise RuntimeError(f"No mask files found under {masks_dir} with suffix {mask_suffix}")

        self.triplets: List[Tuple[Path, Path, Path]] = []  # (cy_path, cn_path, mask_path)
        unmatched: List[str] = []

        for cy_path in cy_imgs:
            cy_stem = cy_path.stem
            
            # CY -> CN 매칭: CY_OY_PC_D000_V3_000 -> CN_OY_PC_D000_V3_000
            cn_stem = cy_stem.replace('CY_', 'CN_')
            cn_path = cn_dir / f"{cn_stem}{cy_path.suffix}"
            
            if not cn_path.exists():
                if self.skip_missing:
                    unmatched.append(f"{cy_stem} (no CN target)")
                    continue
                else:
                    raise FileNotFoundError(f"No CN target found for CY image: {cn_path}")
            
            # 마스크 찾기
            chosen_mask = choose_mask_for_image_stem(cy_stem, mask_files)
            if chosen_mask is not None:
                self.triplets.append((cy_path, cn_path, chosen_mask))
            else:
                if self.skip_missing:
                    unmatched.append(f"{cy_stem} (no mask)")
                else:
                    raise FileNotFoundError(f"No mask found for CY image stem '{cy_stem}'")

        if unmatched:
            print(f"[Warn] {len(unmatched)} CY images had no matching CN target or mask and were skipped.")
            if log_unmatched_to is not None:
                with open(log_unmatched_to, "w") as f:
                    for s in unmatched:
                        f.write(s + "\n")
                print(f"[Info] Unmatched CY images written to: {log_unmatched_to}")

        if not self.triplets:
            raise RuntimeError("After matching, no (CY, CN, mask) triplets were found. Check naming rules.")

        print(f"[Info] Created CY→CN dataset with {len(self.triplets)} triplets")

    def __len__(self):
        return len(self.triplets)

    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str, str, str]:
        cy_path, cn_path, mask_path = self.triplets[idx]
        
        # CY 이미지 (입력)
        cy_img_L = load_grayscale(cy_path, self.image_size)
        cy_x = to_tensor(cy_img_L).float()  # 1xHxW, [0,1]
        
        # CN 이미지 (목표)
        cn_img_L = load_grayscale(cn_path, self.image_size)
        cn_x = to_tensor(cn_img_L).float()  # 1xHxW, [0,1]

        # 채널 조정
        if self.data_channels == 1 and self.model_channels == 3:
            cy_x = cy_x.repeat(3, 1, 1)
            cn_x = cn_x.repeat(3, 1, 1)
        elif self.data_channels == 1 and self.model_channels == 1:
            pass
        elif self.data_channels == 3 and self.model_channels == 3:
            pass
        else:
            raise ValueError(f"Incompatible data/model channels: data={self.data_channels}, model={self.model_channels}")

        # [-1,1] 범위로 변환
        cy_x = data_transform(cy_x)
        cn_x = data_transform(cn_x)

        # 마스크
        m = load_mask_array(mask_path, self.image_size, self.mask_bin_thresh, self.known_is_white)
        if self.invert_mask:
            m = 1.0 - m  # 최종 의미: 1=Known, 0=Unknown
        m_t = torch.from_numpy(m).float()  # HxW

        return cy_x, cn_x, m_t, str(cy_path), str(cn_path), str(mask_path)


class UltrasoundMaskDataset(data.Dataset):
    IMG_EXTS = {".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"}

    def __init__(self, images_dir: str, masks_dir: str, image_size: int,
                 data_channels: int, model_channels: int, mask_suffix: str,
                 mask_bin_thresh: float, known_is_white: bool, invert_mask: bool,
                 skip_missing: bool = True, log_unmatched_to: Optional[Path] = None):
        self.image_size = image_size
        self.data_channels = data_channels
        self.model_channels = model_channels
        self.mask_bin_thresh = mask_bin_thresh
        self.known_is_white = known_is_white
        self.invert_mask = invert_mask
        self.mask_suffix = mask_suffix
        self.skip_missing = skip_missing

        images_dir = Path(images_dir)
        mask_root = Path(masks_dir)

        imgs: List[Path] = []
        for p in sorted(images_dir.iterdir()):
            if p.is_file() and p.suffix.lower() in self.IMG_EXTS:
                imgs.append(p)
        if not imgs:
            raise RuntimeError(f"No images found in {images_dir}")

        mask_files = [Path(x) for x in glob.glob(str(mask_root / "**" / f"*{mask_suffix}"), recursive=True)]
        if not mask_files:
            raise RuntimeError(f"No mask files found under {masks_dir} with suffix {mask_suffix}")

        self.pairs: List[Tuple[Path, Path]] = []
        unmatched: List[str] = []

        for ip in imgs:
            stem = ip.stem
            chosen = choose_mask_for_image_stem(stem, mask_files)
            if chosen is not None:
                self.pairs.append((ip, chosen))
            else:
                if self.skip_missing:
                    unmatched.append(stem)
                else:
                    raise FileNotFoundError(f"No mask found for image stem '{self.mask_suffix}'")

        if unmatched:
            print(f"[Warn] {len(unmatched)} images had no matching mask and were skipped.")
            if log_unmatched_to is not None:
                with open(log_unmatched_to, "w") as f:
                    for s in unmatched:
                        f.write(s + "\n")
                print(f"[Info] Unmatched image stems written to: {log_unmatched_to}")

        if not self.pairs:
            raise RuntimeError("After matching, no (image, mask) pairs were found. Check naming rules.")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, str, str]:
        img_path, mask_path = self.pairs[idx]
        img_L = load_grayscale(img_path, self.image_size)
        x = to_tensor(img_L).float()  # 1xHxW, [0,1]

        if self.data_channels == 1 and self.model_channels == 3:
            x = x.repeat(3, 1, 1)
        elif self.data_channels == 1 and self.model_channels == 1:
            pass
        elif self.data_channels == 3 and self.model_channels == 3:
            pass
        else:
            raise ValueError(f"Incompatible data/model channels: data={self.data_channels}, model={self.model_channels}")

        x = data_transform(x)  # [-1,1]

        m = load_mask_array(mask_path, self.image_size, self.mask_bin_thresh, self.known_is_white)
        if self.invert_mask:
            m = 1.0 - m  # 최종 의미: 1=Known, 0=Unknown
        m_t = torch.from_numpy(m).float()  # HxW

        return x, m_t, str(img_path), str(mask_path)


# =========================
# Diffusers UNet wrapper
# =========================
class DiffusersUNetWrapper(nn.Module):
    def __init__(self, unet: UNet2DModel):
        super().__init__()
        self.unet = unet

    def forward(self, x, t, y=None):
        return self.unet(x, t).sample  # epsilon 예측


# =========================
# 빌더
# =========================
def build_model(cfg: InpaintConfig, device: torch.device) -> nn.Module:
    base_unet = UNet2DModel(
        sample_size=cfg.image_size,
        in_channels=cfg.model_in_channels,
        out_channels=cfg.model_out_channels,
        layers_per_block=cfg.layers_per_block,
        block_out_channels=cfg.block_out_channels,
        down_block_types=cfg.down_block_types,
        up_block_types=cfg.up_block_types,
        attention_head_dim=cfg.attention_head_dim,
    )
    state = torch.load(cfg.ckpt_path, map_location="cpu")
    if isinstance(state, dict) and "model_state_dict" in state:
        sd = state["model_state_dict"]
    elif isinstance(state, dict) and "state_dict" in state:
        sd = state["state_dict"]
    else:
        sd = state
    result = base_unet.load_state_dict(sd, strict=False)
    if getattr(result, "missing_keys", None):
        print(f"Warning: missing keys: {result.missing_keys[:8]} ... ({len(result.missing_keys)} total)")
    if getattr(result, "unexpected_keys", None):
        print(f"Warning: unexpected keys: {result.unexpected_keys[:8]} ... ({len(result.unexpected_keys)} total)")
    base_unet.to(device).eval()
    return DiffusersUNetWrapper(base_unet)

def build_betas(cfg: InpaintConfig, device: torch.device) -> torch.Tensor:
    sch = DDPMScheduler(
        num_train_timesteps=cfg.num_diffusion_timesteps,
        beta_schedule=cfg.beta_schedule,
        prediction_type=cfg.prediction_type,
    )
    return torch.as_tensor(sch.betas, dtype=torch.float32, device=device)

def build_loader(cfg: InpaintConfig, batch_size: int = 1) -> data.DataLoader:
    out_dir = Path(cfg.output_dir)
    ensure_dir(out_dir)
    unmatched_log = out_dir / "unmatched_images.txt"
    
    if cfg.translation_mode:
        # CY→CN 번역 데이터셋 사용
        ds = CYtoCNTranslationDataset(
            cy_images_dir=cfg.images_dir,
            cn_targets_dir=cfg.cn_targets_dir,
            masks_dir=cfg.masks_dir,
            image_size=cfg.image_size,
            data_channels=cfg.data_channels,
            model_channels=cfg.model_in_channels,
            mask_suffix=cfg.mask_suffix,
            mask_bin_thresh=cfg.mask_bin_thresh,
            known_is_white=cfg.known_is_white,
            invert_mask=cfg.invert_mask,
            skip_missing=cfg.skip_missing,
            log_unmatched_to=unmatched_log,
        )
    else:
        # 기존 인페인팅 데이터셋 사용
        ds = UltrasoundMaskDataset(
            images_dir=cfg.images_dir,
            masks_dir=cfg.masks_dir,
            image_size=cfg.image_size,
            data_channels=cfg.data_channels,
            model_channels=cfg.model_in_channels,
            mask_suffix=cfg.mask_suffix,
            mask_bin_thresh=cfg.mask_bin_thresh,
            known_is_white=cfg.known_is_white,
            invert_mask=cfg.invert_mask,
            skip_missing=cfg.skip_missing,
            log_unmatched_to=unmatched_log,
        )
    
    loader = data.DataLoader(
        ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        pin_memory=True,
        drop_last=False,
    )
    return loader


# =========================
# DDRM 본 샘플링
# =========================
@torch.no_grad()
def ddrm_sample_patch(x_init: torch.Tensor, mask01: torch.Tensor, model: nn.Module,
                      betas: torch.Tensor, cfg: InpaintConfig, device: torch.device, patch_size: tuple) -> List[torch.Tensor]:
    """
    DDRM sampling for a patch of a given size.
    """
    x_init = x_init.to(device)
    x_dtype = x_init.dtype

    C = cfg.model_in_channels
    H, W = patch_size

    # 1) 선형 관측자
    mask01_dev = mask01.to(device)
    missing_idx = mask_to_missing_indices(mask01_dev, C, device)
    H_funcs = Inpainting(C, (H, W), missing_indices=missing_idx, device=device)

    # 2) 관측 y0
    y0 = H_funcs.H(x_init)
    if cfg.sigma0 > 0:
        y0 = y0 + torch.randn_like(y0) * cfg.sigma0

    # 3) Unknown만 그레이스케일 노이즈로 초기화
    known01_c, unknown01_c = _build_mask_stacks(mask01_dev, C)
    known01_c = known01_c.to(device=device, dtype=x_dtype)
    unknown01_c = unknown01_c.to(device=device, dtype=x_dtype)

    noise_gray = create_grayscale_noise(x_init.shape, device, std=cfg.init_noise_std, dtype=x_dtype)
    xT = known01_c * x_init + unknown01_c * noise_gray

    # 4) 스텝 시퀀스
    skip = max(1, cfg.num_diffusion_timesteps // cfg.timesteps)
    seq = list(range(0, cfg.num_diffusion_timesteps, skip))[:cfg.timesteps]

    # 5) DDRM 진행
    if cfg.use_ddnm:
        xs, _ = ddnm_steps(
            xT, seq, model, betas, H_funcs, y0, cfg.sigma0,
            etaB=cfg.eta_b, etaA=cfg.eta_a, etaC=cfg.eta_c, cls_fn=None, classes=None
        )
    else:
        xs, _ = efficient_generalized_steps(
            xT, seq, model, betas, H_funcs, y0, cfg.sigma0,
            etaB=cfg.eta_b, etaA=cfg.eta_a, etaC=cfg.eta_c, cls_fn=None, classes=None
        )
    return xs


@torch.no_grad()
def ddrm_sample(x_init: torch.Tensor, mask01: torch.Tensor, model: nn.Module,
                betas: torch.Tensor, cfg: InpaintConfig, device: torch.device) -> List[torch.Tensor]:
    """
    DDRM 샘플링 수행. 반환: 중간 스텝 텐서 리스트(xs).
    - 입력/마스크/노이즈/가중치 텐서를 모두 동일 device/dtype 으로 맞춤.
    """
    x_init = x_init.to(device)
    x_dtype = x_init.dtype

    C = cfg.model_in_channels

    # 1) 선형 관측자
    mask01_dev = mask01.to(device)
    missing_idx = mask_to_missing_indices(mask01_dev, C, device)
    H_funcs = Inpainting(C, (cfg.image_size, cfg.image_size), missing_indices=missing_idx, device=device)

    # 2) 관측 y0
    y0 = H_funcs.H(x_init)
    if cfg.sigma0 > 0:
        y0 = y0 + torch.randn_like(y0) * cfg.sigma0

    # 3) Unknown만 그레이스케일 노이즈로 초기화
    known01_c, unknown01_c = _build_mask_stacks(mask01_dev, C)
    known01_c = known01_c.to(device=device, dtype=x_dtype)
    unknown01_c = unknown01_c.to(device=device, dtype=x_dtype)

    noise_gray = create_grayscale_noise(x_init.shape, device, std=cfg.init_noise_std, dtype=x_dtype)
    xT = known01_c * x_init + unknown01_c * noise_gray

    # 4) 스텝 시퀀스
    skip = max(1, cfg.num_diffusion_timesteps // cfg.timesteps)
    seq = list(range(0, cfg.num_diffusion_timesteps, skip))[:cfg.timesteps]

    # 5) DDRM 진행
    if cfg.use_ddnm:
        xs, _ = ddnm_steps(
            xT, seq, model, betas, H_funcs, y0, cfg.sigma0,
            etaB=cfg.eta_b, etaA=cfg.eta_a, etaC=cfg.eta_c, cls_fn=None, classes=None
        )
    else:
        xs, _ = efficient_generalized_steps(
            xT, seq, model, betas, H_funcs, y0, cfg.sigma0,
            etaB=cfg.eta_b, etaA=cfg.eta_a, etaC=cfg.eta_c, cls_fn=None, classes=None
        )
    return xs


@torch.no_grad()
def ddrm_sample_batch(x_batch: torch.Tensor, mask_batch: torch.Tensor, model: nn.Module,
                      betas: torch.Tensor, cfg: InpaintConfig, device: torch.device) -> List[torch.Tensor]:
    """
    배치로 DDRM 샘플링 수행. VRAM 효율성을 위해 배치 단위로 처리.

    Args:
        x_batch: [B,C,H,W] 배치 입력 이미지
        mask_batch: [B,H,W] 배치 마스크

    Returns:
        List[torch.Tensor]: 각 타임스텝의 결과 [B,C,H,W]
    """
    x_batch = x_batch.to(device)
    x_dtype = x_batch.dtype
    B, C = x_batch.shape[0], cfg.model_in_channels

    # 각 배치 아이템에 대해 개별 처리 (마스크가 다를 수 있음)
    batch_results = []

    for b in range(B):
        x_single = x_batch[b:b+1]  # [1,C,H,W]
        mask_single = mask_batch[b]  # [H,W]

        # 개별 샘플링
        xs_single = ddrm_sample(x_single, mask_single, model, betas, cfg, device)
        batch_results.append([x[0:1] for x in xs_single])  # 각 타임스텝을 [1,C,H,W]로 유지

    # 배치로 재결합
    xs_batch = []
    num_steps = len(batch_results[0])
    for step in range(num_steps):
        step_batch = torch.cat([batch_results[b][step] for b in range(B)], dim=0)
        xs_batch.append(step_batch)

    return xs_batch


def process_batch(x_batch: torch.Tensor, mask_batch: torch.Tensor, model: nn.Module,
                  betas: torch.Tensor, cfg: InpaintConfig, device: torch.device, 
                  cn_target_batch: Optional[torch.Tensor] = None) -> torch.Tensor:
    """
    배치 단위로 전체 파이프라인(2-패스 + 정제) 처리

    Returns:
        torch.Tensor: 최종 결과 배치 [B,C,H,W]
    """
    B = x_batch.shape[0]
    C = cfg.model_in_channels

    final_results = []

    for b in range(B):
        x_single = x_batch[b:b+1]  # [1,C,H,W]
        mask_single = mask_batch[b]  # [H,W]

        # 패스 1: Unknown 팽창 마스크
        if cfg.mask_dilate_px > 0:
            mask_pass1 = dilate_unknown_mask(mask_single, cfg.mask_dilate_px)
        else:
            mask_pass1 = mask_single.clone()

        xs_pass1 = ddrm_sample(x_single, mask_pass1, model, betas, cfg, device)
        x_pass1_last = xs_pass1[-1].detach()

        # 패스 2: 원래 마스크
        xs_pass2 = ddrm_sample(x_pass1_last, mask_single, model, betas, cfg, device)
        x_pass2_last = xs_pass2[-1].detach()

        # 정제 사용 시
        if cfg.do_refine:
            x_final = refine_unknown_with_guidance(
                x_in=x_pass2_last, x_context=x_single, mask01=mask_single, cfg=cfg
            )
        else:
            x_final = x_pass2_last

        final_results.append(x_final)

        # 메모리 정리
        del xs_pass1, xs_pass2, x_pass1_last, x_pass2_last
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # 추가 메모리 최적화
        if hasattr(torch.cuda, 'reset_peak_memory_stats'):
            torch.cuda.reset_peak_memory_stats()

    return torch.cat(final_results, dim=0)


# =========================
# 사후 정제(Unknown만 최적화)
# =========================
def gaussian_blur(img: torch.Tensor, k: int = 15, sigma: float = 5.0) -> torch.Tensor:
    coords = torch.arange(k, device=img.device, dtype=img.dtype) - (k - 1) / 2
    ker1 = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
    ker1 = (ker1 / ker1.sum()).view(1, 1, -1, 1)
    ker2 = ker1.transpose(2, 3)
    ch = img.shape[1]
    img = F.conv2d(img, ker1.repeat(ch, 1, 1, 1), padding=(k // 2, 0), groups=ch)
    img = F.conv2d(img, ker2.repeat(ch, 1, 1, 1), padding=(0, k // 2), groups=ch)
    return img

def total_variation(x: torch.Tensor) -> torch.Tensor:
    dx = x[:, :, :, 1:] - x[:, :, :, :-1]
    dy = x[:, :, 1:, :] - x[:, :, :-1, :]
    return (dx.abs().mean() + dy.abs().mean())

def match_mean_std_loss(x_u: torch.Tensor, ref_k: torch.Tensor) -> torch.Tensor:
    eps = 1e-6
    def stats(t):
        s = t.sum(dim=[2,3], keepdim=True)
        n = (t != 0).float().sum(dim=[2,3], keepdim=True).clamp_min(1.0)
        mean = s / n
        var = ((t - mean) * (t != 0).float()).pow(2).sum(dim=[2,3], keepdim=True) / n.clamp_min(1.0)
        std = (var + eps).sqrt()
        return mean, std
    mu_u, std_u = stats(x_u)
    mu_k, std_k = stats(ref_k)
    return (mu_u - mu_k).abs().mean() + (std_u - std_k).abs().mean()

def refine_unknown_with_guidance(x_in: torch.Tensor, x_context: torch.Tensor, mask01: torch.Tensor,
                                 cfg: InpaintConfig) -> torch.Tensor:
    """
    x_in: DDRM 출력(1xCxHxW, [-1,1])
    x_context: 원본 입력(1xCxHxW, [-1,1])
    mask01: 1=Known, 0=Unknown
    Unknown 영역만 업데이트(Adam)하며 다음 손실 최소화:
      - 저주파 일치(가우시안 블러)
      - 히스토그램(mean/std) 일치(Unknown vs Known)
      - TV(Unknown)
      - 경계 링(seam)에서의 컨텍스트 근접
    """
    device = x_in.device
    dtype = x_in.dtype
    C = x_in.shape[1]

    # === 모든 텐서를 같은 device/dtype으로 강제 정렬 (device mismatch 오류 방지) ===
    x = x_in.clone().to(device=device, dtype=dtype).detach().requires_grad_(True)
    x_context = x_context.to(device=device, dtype=dtype)
    mask01_d = mask01.to(device=device, dtype=dtype)

    known01_c, unknown01_c = _build_mask_stacks(mask01_d, C)
    known01_c = known01_c.to(device=device, dtype=dtype)
    unknown01_c = unknown01_c.to(device=device, dtype=dtype)

    seam_ring = make_seam_ring(mask01_d, cfg.seam_width)[None, None, :, :].repeat(1, C, 1, 1)
    seam_ring = seam_ring.to(device=device, dtype=dtype)

    opt = torch.optim.Adam([x], lr=cfg.refine_lr)

    # 저주파 타깃: 원본의 블러
    target_lfreq = gaussian_blur(x_context, k=15, sigma=5.0).detach()

    for _ in range(cfg.refine_iters):
        opt.zero_grad()
        # Unknown/Known 마스크 적용
        x_u = x * unknown01_c
        x_k = x_context * known01_c

        # 1) 저주파 일치
        l_lf = F.mse_loss(gaussian_blur(x), target_lfreq)

        # 2) 히스토그램(mean/std) 일치(Unknown 대비 Known)
        l_hist = match_mean_std_loss(x_u, x_k)

        # 3) TV(Unknown)
        l_tv = total_variation(x * unknown01_c)

        # 4) 경계 링에서 컨텍스트 근접
        l_seam = F.l1_loss(x * seam_ring, x_context * seam_ring)

        loss = cfg.w_lfreq * l_lf + cfg.w_hist * l_hist + cfg.w_tv * l_tv + cfg.w_seam * l_seam
        loss.backward()

        # Unknown만 업데이트되도록 gradient mask
        with torch.no_grad():
            if x.grad is not None:
                x.grad.mul_(unknown01_c)
        opt.step()

        with torch.no_grad():
            # Known은 항상 컨텍스트로 고정
            x.data = known01_c * x_context + unknown01_c * x.data
            x.data.clamp_(-1.0, 1.0)

    return x.detach()


# =========================
# 저장
# =========================
def save_outputs(xs: List[torch.Tensor], x_orig: torch.Tensor, pinv_y0: torch.Tensor,
                 stem: str, out_dir: Path, cfg: InpaintConfig, tag: str = ""):
    # y0 저장(옵션)
    if cfg.save_y0:
        y0_img = inverse_data_transform(pinv_y0[0].detach().cpu())
        if y0_img.shape[0] == 3:
            y0_img = y0_img.mean(dim=0, keepdim=True)
        tvu.save_image(y0_img, str(out_dir / f"{stem}{tag}_y0.png"))

    # 원본 저장(옵션)
    if cfg.save_orig:
        orig_img = inverse_data_transform(x_orig[0].detach().cpu())
        if orig_img.shape[0] == 3:
            orig_img = orig_img.mean(dim=0, keepdim=True)
        tvu.save_image(orig_img, str(out_dir / f"{stem}{tag}_orig.png"))

    # 마지막 결과는 항상 저장
    x_last = inverse_data_transform(xs[-1]).clamp(0.0, 1.0)
    if x_last.shape[1] == 3:
        x_last = x_last.mean(dim=1, keepdim=True)
    tvu.save_image(x_last[0].cpu(), str(out_dir / f"{stem}{tag}_ddrm.png"))

    # --save-all-steps 인 경우 스텝 프레임 추가 저장
    if not cfg.save_last_only:
        for k, xk in enumerate(xs):
            if k == 0:
                continue  # 초기화 프레임(x_T) 스킵
            if k % cfg.step_interval == 0:
                xk = inverse_data_transform(xk).clamp(0.0, 1.0)
                if xk.shape[1] == 3:
                    xk = xk.mean(dim=1, keepdim=True)
                tvu.save_image(xk[0].cpu(), str(out_dir / f"{stem}{tag}_step{k:03d}.png"))


# =========================
# 단일 샘플 전체 파이프라인(패스/정제 포함)
# =========================
def process_one_patch_based(x_orig: torch.Tensor, mask01: torch.Tensor, model: nn.Module,
                            betas: torch.Tensor, cfg: InpaintConfig, device: torch.device,
                            out_dir: Path, stem: str, ts_label: str,
                            do_save: bool = True, return_last: bool = False) -> Optional[torch.Tensor]:

    C, H, W = x_orig.shape[1], x_orig.shape[2], x_orig.shape[3]
    DIVISIBILITY_FACTOR = 32 # 2**num_downsampling_layers

    # Accumulators for the final image and patch counts
    final_image_sum = torch.zeros_like(x_orig)
    patch_counts = torch.zeros_like(x_orig)

    # Get the bounding box of the unknown region to focus processing
    unknown_indices = (mask01 < 0.5).nonzero()
    if unknown_indices.numel() == 0: # No unknown areas
        if return_last:
            return x_orig
        return None

    min_y, min_x = unknown_indices.min(dim=0)[0]
    max_y, max_x = unknown_indices.max(dim=0)[0]

    # Iterate over patches
    for y in range(min_y, max_y + 1, cfg.patch_stride):
        for x in range(min_x, max_x + 1, cfg.patch_stride):
            # Define patch boundaries
            y_start, x_start = y, x
            y_end, x_end = min(y + cfg.patch_size, H), min(x + cfg.patch_size, W)

            patch_x_orig = x_orig[:, :, y_start:y_end, x_start:x_end]
            patch_mask01 = mask01[y_start:y_end, x_start:x_end]

            # Pad the patch to be divisible by the factor
            original_H, original_W = patch_x_orig.shape[2], patch_x_orig.shape[3]
            target_H = ((original_H - 1) // DIVISIBILITY_FACTOR + 1) * DIVISIBILITY_FACTOR
            target_W = ((original_W - 1) // DIVISIBILITY_FACTOR + 1) * DIVISIBILITY_FACTOR

            pad_H = target_H - original_H
            pad_W = target_W - original_W

            # (pad_left, pad_right, pad_top, pad_bottom)
            padding = (0, pad_W, 0, pad_H)
            padded_patch_x_orig = F.pad(patch_x_orig, padding, mode='reflect')
            padded_patch_mask01 = F.pad(patch_mask01.unsqueeze(0).unsqueeze(0), padding, mode='reflect').squeeze(0).squeeze(0)

            # Run DDRM on the padded patch
            patch_result_xs = ddrm_sample_patch(padded_patch_x_orig, padded_patch_mask01, model, betas, cfg, device, patch_size=(target_H, target_W))
            patch_final_padded = patch_result_xs[-1]

            # Crop the result back to the original patch size
            patch_final = patch_final_padded[:, :, :original_H, :original_W]

            # Accumulate results
            final_image_sum[:, :, y_start:y_end, x_start:x_end] += patch_final
            patch_counts[:, :, y_start:y_end, x_start:x_end] += 1

    # Average the results where patches overlapped
    final_image = final_image_sum / patch_counts.clamp(min=1)

    # The known areas should be from the original image
    known01_c, _ = _build_mask_stacks(mask01, C)
    final_image = known01_c.to(device) * x_orig + (1 - known01_c.to(device)) * final_image

    if do_save:
        # For saving, we can use the existing save_outputs function
        # We pass the final image as a list containing just the last result
        pinv_y0 = torch.zeros_like(x_orig) # Placeholder for y0
        save_outputs([final_image], x_orig, pinv_y0, stem, out_dir, cfg, tag=f"_{ts_label}_patched")

    if return_last:
        return final_image

    return None


def process_one(x_orig: torch.Tensor, mask01: torch.Tensor, model: nn.Module,
                betas: torch.Tensor, cfg: InpaintConfig, device: torch.device,
                out_dir: Path, stem: str, ts_label: str,
                do_save: bool = True, return_last: bool = False) -> Optional[torch.Tensor]:
    if cfg.use_patch_inference:
        return process_one_patch_based(x_orig, mask01, model, betas, cfg, device, out_dir, stem, ts_label, do_save, return_last)
    """
    2-패스(어닐링) + (선택) 사후 정제
    - 2차 패스 전체 궤적 저장(스텝/마지막)
    - 정제 사용 시, 정제 결과는 별도 파일로 추가 저장
    - return_last=True면 마지막 결과 텐서를 반환(저장 비활성도 선택 가능)
    """
    C = cfg.model_in_channels

    # pinv 시각화용
    missing_idx0 = mask_to_missing_indices(mask01, C, device)
    H_funcs0 = Inpainting(C, cfg.image_size, missing_indices=missing_idx0, device=device)
    with torch.no_grad():
        y0 = H_funcs0.H(x_orig)
        if cfg.sigma0 > 0:
            y0 = y0 + torch.randn_like(y0) * cfg.sigma0
        pinv_y0 = H_funcs0.H_pinv(y0).view(
            y0.shape[0], C, cfg.image_size, cfg.image_size
        )

    # 패스 1: Unknown 팽창 마스크
    if cfg.mask_dilate_px > 0:
        mask_pass1 = dilate_unknown_mask(mask01, cfg.mask_dilate_px)
    else:
        mask_pass1 = mask01.clone()

    xs_pass1 = ddrm_sample(x_orig, mask_pass1, model, betas, cfg, device)
    x_pass1_last = xs_pass1[-1].detach()

    # 패스 2: 원래 마스크
    xs_pass2 = ddrm_sample(x_pass1_last, mask01, model, betas, cfg, device)
    x_pass2_last = xs_pass2[-1].detach()

    # 정제 사용 시
    if cfg.do_refine:
        x_final = refine_unknown_with_guidance(
            x_in=x_pass2_last, x_context=x_orig, mask01=mask01, cfg=cfg
        )
    else:
        x_final = x_pass2_last

    tag = f"_{ts_label}"

    if do_save:
        # 2차 패스 궤적 저장(항상 마지막 + 필요시 스텝)
        save_outputs([x_final], x_orig, pinv_y0, stem, out_dir, cfg, tag=tag)

    if return_last:
        return x_final
    return None


# =========================
# 실행: 표준 배치 처리
# =========================
def run_ddrm_inpaint(cfg: InpaintConfig):
    set_seed(cfg.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = build_model(cfg, device)
    betas = build_betas(cfg, device)
    loader = build_loader(cfg, batch_size=1)  # 표준 실행은 배치 크기 1

    print(f"[Info] Total matched pairs: {len(loader.dataset)}")
    print(f"[Info] init_noise_std={cfg.init_noise_std}, invert_mask={cfg.invert_mask}, mask_dilate_px={cfg.mask_dilate_px}")
    print(f"[Info] refine(do_refine={cfg.do_refine}): iters={cfg.refine_iters}, "
          f"w_lfreq={cfg.w_lfreq}, w_hist={cfg.w_hist}, w_tv={cfg.w_tv}, w_seam={cfg.w_seam}")

    for timesteps in cfg._timesteps_list:
        ts_output_dir = Path(cfg.output_dir) / f"ts_{timesteps}_anneal{cfg.mask_dilate_px}_ref{int(cfg.do_refine)}"
        ensure_dir(ts_output_dir)

        original_timesteps = cfg.timesteps
        cfg.timesteps = timesteps

        for idx, (x_orig, mask01, img_path, mask_path) in enumerate(loader, 1):
            x_orig = x_orig.to(device)
            mask01 = mask01[0].to(device)  # HxW
            img_path0 = img_path[0] if isinstance(img_path, (list, tuple)) else img_path
            stem = Path(img_path0).stem

            known_ratio = (mask01 > 0.5).float().mean().item()
            print(f"[{idx}/{len(loader.dataset)}] {stem}  known_ratio={known_ratio:.3f}  ts={timesteps}")

            process_one(
                x_orig=x_orig, mask01=mask01, model=model, betas=betas, cfg=cfg,
                device=device, out_dir=ts_output_dir, stem=stem, ts_label=str(timesteps),
                do_save=True, return_last=False
            )

        cfg.timesteps = original_timesteps

    print("Done.")


# =========================
# Optuna: orig와 유사(MAE@Unknown) 최소화
# =========================
def mae_unknown(pred: torch.Tensor, gt: torch.Tensor, mask01: torch.Tensor) -> float:
    """
    pred, gt: [1,C,H,W] in [-1,1]
    mask01: [H,W] 1=Known, 0=Unknown
    반환: Unknown 영역 MAE (float)
    """
    device = pred.device
    dtype = pred.dtype

    # 모두 같은 device/dtype으로 정렬
    pred = pred.to(device=device, dtype=dtype)
    gt = gt.to(device=device, dtype=dtype)
    mask01 = mask01.to(device=device, dtype=dtype)

    pred01 = inverse_data_transform(pred).clamp(0.0, 1.0)
    gt01 = inverse_data_transform(gt).clamp(0.0, 1.0)

    if pred01.shape[1] == 3:
        pred01 = pred01.mean(dim=1, keepdim=True)
    if gt01.shape[1] == 3:
        gt01 = gt01.mean(dim=1, keepdim=True)

    unk = (mask01 < 0.5).to(device=device, dtype=dtype)[None, None, :, :]
    err = (pred01 - gt01).abs() * unk
    denom = unk.sum().clamp_min(torch.tensor(1.0, device=device, dtype=dtype))

    return float((err.sum() / denom).item())

def mae_translation(pred: torch.Tensor, cn_target: torch.Tensor) -> float:
    """
    CY→CN 번역 목표: 전체 이미지에 대한 MAE 계산
    pred: DDRM으로 처리된 CY 이미지 [1,C,H,W] in [-1,1]
    cn_target: 목표 CN 이미지 [1,C,H,W] in [-1,1]
    반환: 전체 이미지 MAE (float)
    """
    device = pred.device
    dtype = pred.dtype

    # 모두 같은 device/dtype으로 정렬
    pred = pred.to(device=device, dtype=dtype)
    cn_target = cn_target.to(device=device, dtype=dtype)

    pred01 = inverse_data_transform(pred).clamp(0.0, 1.0)
    target01 = inverse_data_transform(cn_target).clamp(0.0, 1.0)

    if pred01.shape[1] == 3:
        pred01 = pred01.mean(dim=1, keepdim=True)
    if target01.shape[1] == 3:
        target01 = target01.mean(dim=1, keepdim=True)

    err = (pred01 - target01).abs()
    return float(err.mean().item())


def mae_unknown_batch(pred_batch: torch.Tensor, gt_batch: torch.Tensor, mask_batch: torch.Tensor) -> float:
    """
    배치 단위로 Unknown 영역 MAE 계산

    Args:
        pred_batch: [B,C,H,W] in [-1,1]
        gt_batch: [B,C,H,W] in [-1,1]
        mask_batch: [B,H,W] 1=Known, 0=Unknown

    Returns:
        float: 배치 전체 평균 MAE
    """
    device = pred_batch.device
    dtype = pred_batch.dtype

    # 모두 같은 device/dtype으로 정렬
    pred_batch = pred_batch.to(device=device, dtype=dtype)
    gt_batch = gt_batch.to(device=device, dtype=dtype)
    mask_batch = mask_batch.to(device=device, dtype=dtype)

    pred01 = inverse_data_transform(pred_batch).clamp(0.0, 1.0)
    gt01 = inverse_data_transform(gt_batch).clamp(0.0, 1.0)

    if pred01.shape[1] == 3:
        pred01 = pred01.mean(dim=1, keepdim=True)
    if gt01.shape[1] == 3:
        gt01 = gt01.mean(dim=1, keepdim=True)

    # [B,H,W] -> [B,1,H,W]
    unk = (mask_batch < 0.5).to(device=device, dtype=dtype).unsqueeze(1)
    err = (pred01 - gt01).abs() * unk

    # 배치 전체에서 평균 계산
    total_err = err.sum()
    total_pixels = unk.sum().clamp_min(torch.tensor(1.0, device=device, dtype=dtype))

    return float((total_err / total_pixels).item())

def mae_translation_batch(pred_batch: torch.Tensor, cn_target_batch: torch.Tensor) -> float:
    """
    배치 단위로 CY→CN 번역 MAE 계산

    Args:
        pred_batch: DDRM으로 처리된 CY 이미지들 [B,C,H,W] in [-1,1]
        cn_target_batch: 목표 CN 이미지들 [B,C,H,W] in [-1,1]

    Returns:
        float: 배치 전체 평균 MAE
    """
    device = pred_batch.device
    dtype = pred_batch.dtype

    # 모두 같은 device/dtype으로 정렬
    pred_batch = pred_batch.to(device=device, dtype=dtype)
    cn_target_batch = cn_target_batch.to(device=device, dtype=dtype)

    pred01 = inverse_data_transform(pred_batch).clamp(0.0, 1.0)
    target01 = inverse_data_transform(cn_target_batch).clamp(0.0, 1.0)

    if pred01.shape[1] == 3:
        pred01 = pred01.mean(dim=1, keepdim=True)
    if target01.shape[1] == 3:
        target01 = target01.mean(dim=1, keepdim=True)

    err = (pred01 - target01).abs()
    return float(err.mean().item())

def run_optuna(cfg: InpaintConfig):
    if optuna is None:
        raise ImportError("Optuna가 설치되어 있지 않습니다. 먼저 `pip install optuna`를 수행하세요.")

    set_seed(cfg.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 모델/스케줄러/로더(저장 최소화 위해 저장 옵션 OFF로 복사)
    base_cfg = replace(cfg, save_y0=False, save_orig=False, save_last_only=True)
    model = build_model(base_cfg, device)
    betas = build_betas(base_cfg, device)

    # 배치 크기로 로더 생성 (24GB VRAM에서 더 큰 배치 지원)
    max_batch = 12 if base_cfg.optuna_aggressive_memory else 8
    batch_size = min(base_cfg.optuna_batch_size, max_batch)
    loader = build_loader(base_cfg, batch_size=batch_size)

    # 평가에 사용할 샘플 수 (배치 단위로 조정)
    N = min(base_cfg.optuna_samples, len(loader.dataset))
    N_batches = (N + batch_size - 1) // batch_size

    aggressive_str = " (aggressive)" if base_cfg.optuna_aggressive_memory else ""
    print(f"[Optuna] trials={base_cfg.optuna_trials}, samples={N}, batch_size={batch_size}{aggressive_str}, batches={N_batches}")

    # 초기 GPU 메모리 상태 출력
    mem_info = get_gpu_memory_info()
    if mem_info:
        print(f"[GPU Memory] Initial - Allocated: {mem_info['allocated']:.2f}GB, Reserved: {mem_info['reserved']:.2f}GB")

    def objective(trial) -> float:
        # ----- 탐색 공간 -----
        timesteps = trial.suggest_int("timesteps", 500, 800, step=20)
        eta_b = trial.suggest_float("eta_b", 0.0, 1.0)
        eta_c = trial.suggest_float("eta_c", 0.0, 1.0)
        init_noise_std = trial.suggest_float("init_noise_std", 0.2, 1.0)
        mask_dilate_px = trial.suggest_int("mask_dilate_px", 0, 8)

        # do_refine을 반반으로 나누어 배정 (짝수: False, 홀수: True)
        do_refine = (trial.number % 2) == 1
        refine_iters = trial.suggest_int("refine_iters", 30, 200)
        refine_lr = trial.suggest_float("refine_lr", 0.02, 0.1)
        w_lfreq = trial.suggest_float("w_lfreq", 0.2, 2.0)
        w_hist = trial.suggest_float("w_hist", 0.05, 0.4)
        w_tv = trial.suggest_float("w_tv", 0.0005, 0.005)
        w_seam = trial.suggest_float("w_seam", 0.3, 1.5)
        seam_width = trial.suggest_int("seam_width", 1, 4)

        # ----- trial용 cfg 구성 -----
        tcfg = replace(
            base_cfg,
            timesteps=timesteps,
            eta_b=eta_b, eta_c=eta_c,
            init_noise_std=init_noise_std,
            mask_dilate_px=mask_dilate_px,
            do_refine=do_refine,
            refine_iters=refine_iters,
            refine_lr=refine_lr,
            w_lfreq=w_lfreq,
            w_hist=w_hist,
            w_tv=w_tv,
            w_seam=w_seam,
            seam_width=seam_width,
            _timesteps_list=(timesteps,),  # optuna에선 단일 timesteps로 평가
        )

        # ----- 배치 단위로 평균 MAE(Unknown) -----
        total_score = 0.0
        total_samples = 0

        # Trial 시작 시 메모리 상태
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

        with torch.no_grad():
            pass  # placeholder to emphasize we leave grad off except refine step

        for batch_idx, batch_data in enumerate(loader):
            if batch_idx >= N_batches:
                break

            if base_cfg.translation_mode:
                # CY→CN 번역 모드: (cy_x, cn_x, mask, cy_path, cn_path, mask_path)
                cy_batch, cn_batch, mask_batch, _, _, _ = batch_data
                current_batch_size = cy_batch.shape[0]
                
                cy_batch = cy_batch.to(device)
                cn_batch = cn_batch.to(device) 
                mask_batch = mask_batch.to(device)  # [B,H,W]
                
                # 배치 처리 (CY 이미지로 DDRM 수행)
                x_final_batch = process_batch(cy_batch, mask_batch, model, betas, tcfg, device, cn_batch)
                
                # CY→CN 번역 MAE 계산 (전체 이미지 기준)
                batch_score = mae_translation_batch(x_final_batch, cn_batch)
                
                # 메모리 안정화
                del x_final_batch, cy_batch, cn_batch, mask_batch
            else:
                # 기존 인페인팅 모드: (x, mask, img_path, mask_path)
                x_batch, mask_batch, _, _ = batch_data
                current_batch_size = x_batch.shape[0]
                
                x_batch = x_batch.to(device)
                mask_batch = mask_batch.to(device)  # [B,H,W]
                
                # 배치 처리
                x_final_batch = process_batch(x_batch, mask_batch, model, betas, tcfg, device)
                
                # 기존 방식: Unknown 영역 MAE 계산
                batch_score = mae_unknown_batch(x_final_batch, x_batch, mask_batch)
                
                # 메모리 안정화
                del x_final_batch, x_batch, mask_batch
                
            total_score += batch_score * current_batch_size
            total_samples += current_batch_size
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        mean_score = total_score / max(total_samples, 1)

        # Trial 종료 시 메모리 사용량 및 do_refine 상태 출력
        mem_info = get_gpu_memory_info()
        refine_str = "refine" if do_refine else "no-refine"
        if mem_info:
            print(f"[Trial {trial.number}] MAE: {mean_score:.4f}, GPU Peak: {mem_info['max_allocated']:.2f}GB, {refine_str}")

        return mean_score

    # 병렬 처리 설정
    if base_cfg.optuna_n_jobs > 1:
        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=base_cfg.optuna_trials, n_jobs=base_cfg.optuna_n_jobs, catch=(RuntimeError,))
    else:
        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=base_cfg.optuna_trials, catch=(RuntimeError,))


    # do_refine True/False별로 best trial 분리
    refine_trials = []
    no_refine_trials = []
    
    for trial in study.trials:
        if trial.state == optuna.trial.TrialState.COMPLETE:
            # trial number로 do_refine 값 결정 (objective 함수와 동일한 로직)
            do_refine = (trial.number % 2) == 1
            if do_refine:
                refine_trials.append(trial)
            else:
                no_refine_trials.append(trial)
    
    out_dir = Path(base_cfg.output_dir)
    ensure_dir(out_dir)
    
    print(f"\n[Optuna] Results: {len(refine_trials)} refine trials, {len(no_refine_trials)} no-refine trials")
    
    # do_refine=True 중 best
    if refine_trials:
        best_refine = min(refine_trials, key=lambda t: t.value)
        print(f"\n[Best with REFINE]:")
        print(f"  value: {best_refine.value:.6f}")
        print(f"  trial: {best_refine.number}")
        print("  params:")
        for k, v in best_refine.params.items():
            print(f"    {k}: {v}")
        
        # refine=True best params 저장
        with open(out_dir / "optuna_best_params_refine.txt", "w") as f:
            f.write(f"# Best parameters for do_refine=True\n")
            f.write(f"best_value={best_refine.value}\n")
            f.write(f"trial_number={best_refine.number}\n")
            f.write(f"do_refine=True\n")
            for k, v in best_refine.params.items():
                f.write(f"{k}={v}\n")
    
    # do_refine=False 중 best  
    if no_refine_trials:
        best_no_refine = min(no_refine_trials, key=lambda t: t.value)
        print(f"\n[Best WITHOUT REFINE]:")
        print(f"  value: {best_no_refine.value:.6f}")
        print(f"  trial: {best_no_refine.number}")
        print("  params:")
        for k, v in best_no_refine.params.items():
            print(f"    {k}: {v}")
        
        # refine=False best params 저장
        with open(out_dir / "optuna_best_params_no_refine.txt", "w") as f:
            f.write(f"# Best parameters for do_refine=False\n")
            f.write(f"best_value={best_no_refine.value}\n")
            f.write(f"trial_number={best_no_refine.number}\n")
            f.write(f"do_refine=False\n")
            for k, v in best_no_refine.params.items():
                f.write(f"{k}={v}\n")
    
    # 전체 best (기존 호환성을 위해 유지)
    print(f"\n[Overall Best]:")
    print(f"  value: {study.best_trial.value:.6f}")
    print(f"  trial: {study.best_trial.number}")
    overall_do_refine = (study.best_trial.number % 2) == 1
    print(f"  do_refine: {overall_do_refine}")
    
    # 전체 best params 저장 (기존)
    with open(out_dir / "optuna_best_params.txt", "w") as f:
        f.write(f"# Overall best parameters\n")
        f.write(f"best_value={study.best_trial.value}\n")
        f.write(f"trial_number={study.best_trial.number}\n")
        f.write(f"do_refine={overall_do_refine}\n")
        for k, v in study.best_trial.params.items():
            f.write(f"{k}={v}\n")
    
    # 비교 분석 결과 저장
    if refine_trials and no_refine_trials:
        refine_improvement = ((best_no_refine.value - best_refine.value) / best_no_refine.value) * 100
        with open(out_dir / "optuna_comparison.txt", "w") as f:
            f.write(f"# Refine vs No-Refine Comparison\n")
            f.write(f"best_refine_value={best_refine.value:.6f}\n")
            f.write(f"best_no_refine_value={best_no_refine.value:.6f}\n")
            f.write(f"refine_improvement_percent={refine_improvement:.2f}%\n")
            f.write(f"refine_is_better={best_refine.value < best_no_refine.value}\n")
        
        print(f"\n[Comparison]:")
        print(f"  Refine improvement: {refine_improvement:.2f}%")
        print(f"  Refine is better: {best_refine.value < best_no_refine.value}")


# =========================
# CLI
# =========================
def build_argparser():
    p = argparse.ArgumentParser("Ultrasound DDRM Inpainting (diffusers UNet2DModel) — annealing + post refinement + Optuna")
    # 경로
    p.add_argument("--images-dir", type=str, required=True)
    p.add_argument("--masks-dir", type=str, required=True)
    p.add_argument("--output-dir", type=str, required=True)
    p.add_argument("--ckpt-path", type=str, required=True)
    
    # CY→CN 번역 모드
    p.add_argument("--translation-mode", action="store_true", help="CY→CN 이미지 번역 모드 사용")
    p.add_argument("--cn-targets-dir", type=str, default="/home/juneyonglee/Desktop/ddrm/datasets/test_CN", 
                   help="CN 타겟 이미지 디렉토리")

    # 데이터/모델
    p.add_argument("--image-size", type=int, default=512)
    p.add_argument("--data-channels", type=int, default=1, choices=[1, 3])
    p.add_argument("--model-in-channels", type=int, default=3, choices=[1, 3])
    p.add_argument("--model-out-channels", type=int, default=3, choices=[1, 3])

    # diffusers UNet 구조
    p.add_argument("--layers-per-block", type=int, default=2)
    p.add_argument("--block-out-channels", type=str, default="128,256,512,512,1024")
    p.add_argument("--down-block-types", type=str,
                   default="DownBlock2D,DownBlock2D,DownBlock2D,AttnDownBlock2D,DownBlock2D")
    p.add_argument("--up-block-types", type=str,
                   default="UpBlock2D,AttnUpBlock2D,UpBlock2D,UpBlock2D,UpBlock2D")
    p.add_argument("--attention-head-dim", type=int, default=8)

    # 샘플링
    p.add_argument("--timesteps", type=int, default=80)  # 단일 실행시 사용(내부에선 timesteps-list 우선)
    p.add_argument("--timesteps-list", type=str, default="500,700",
                   help="콤마로 구분된 timesteps 리스트. 예: 80,200")
    p.add_argument("--eta-a", type=float, default=0.0)
    p.add_argument("--eta-b", type=float, default=0.3)
    p.add_argument("--eta-c", type=float, default=0.6)
    p.add_argument("--sigma0", type=float, default=0.0)

    # 로더/일반
    p.add_argument("--num-workers", type=int, default=4)
    p.add_argument("--seed", type=int, default=123)

    # 마스크
    p.add_argument("--mask-suffix", type=str, default="_noaug.jpg")
    p.add_argument("--mask-bin-thresh", type=float, default=0.5)
    p.add_argument("--known-is-white", action="store_true", default=True)

    p.add_argument("--mask-dilate-px", type=int, default=2, help="1차 패스 Unknown 팽창 픽셀 수(0=비활성)")

    # 저장
    p.add_argument("--no-save-y0", action="store_true")
    p.add_argument("--no-save-orig", action="store_true")
    p.add_argument("--save-all-steps", action="store_true")
    p.add_argument("--step-interval", type=int, default=100)

    # 베타 스케줄(diffusers와 일치)
    p.add_argument("--num-diffusion-timesteps", type=int, default=1000)
    p.add_argument("--beta-schedule", type=str, default="scaled_linear",
                   choices=["linear", "scaled_linear", "squaredcos_cap_v2", "sigmoid"])
    p.add_argument("--prediction-type", type=str, default="epsilon",
                   choices=["epsilon", "sample", "v_prediction"])

    # 초기화 제어
    p.add_argument("--init-noise-std", type=float, default=0.5,
                   help="Unknown(마스크=0) 영역에 사용할 그레이스케일 가우시안 초기화 표준편차")

    # 사후 정제
    p.add_argument("--do-refine", action="store_true", help="사후 정제(저주파/히스토그램/TV/경계) 수행")
    p.add_argument("--refine-iters", type=int, default=100)
    p.add_argument("--refine-lr", type=float, default=0.05)
    p.add_argument("--w-lfreq", type=float, default=0.6)
    p.add_argument("--w-hist", type=float, default=0.2)
    p.add_argument("--w-tv", type=float, default=0.0015)
    p.add_argument("--w-seam", type=float, default=0.9)
    p.add_argument("--seam-width", type=int, default=2)

    # Optuna
    p.add_argument("--optuna", action="store_true", help="Optuna로 하이퍼파라미터 최적화 수행")
    p.add_argument("--optuna-trials", type=int, default=30)
    p.add_argument("--optuna-samples", type=int, default=6)
    p.add_argument("--optuna-batch-size", type=int, default=4, help="Optuna 평가 시 배치 크기 (VRAM 사용량 증가)")
    p.add_argument("--optuna-n-jobs", type=int, default=2, help="Optuna 병렬 작업 수 (GPU 메모리가 충분한 경우만)")
    p.add_argument("--optuna-aggressive-memory", action="store_true", help="더 공격적인 메모리 사용 (배치 크기 최대 12까지)")



    # Patch-based inference
    p.add_argument("--use-patch-inference", action="store_true", help="Use patch-based inference")
    p.add_argument("--patch-size", type=int, default=256, help="Patch size for patch-based inference")
    p.add_argument("--patch-stride", type=int, default=128, help="Patch stride for patch-based inference")

    return p

def args_to_config(args: argparse.Namespace) -> InpaintConfig:
    boc = tuple(int(x) for x in args.block_out_channels.split(",") if x.strip() != "")
    dbt = tuple(x.strip() for x in args.down_block_types.split(",") if x.strip() != "")
    ubt = tuple(x.strip() for x in args.up_block_types.split(",") if x.strip() != "")
    cfg = InpaintConfig(
        images_dir=args.images_dir,
        masks_dir=args.masks_dir,
        output_dir=args.output_dir,
        ckpt_path=args.ckpt_path,
        translation_mode=args.translation_mode,
        cn_targets_dir=args.cn_targets_dir,
        image_size=args.image_size,
        data_channels=args.data_channels,
        model_in_channels=args.model_in_channels,
        model_out_channels=args.model_out_channels,
        layers_per_block=args.layers_per_block,
        block_out_channels=boc,
        down_block_types=dbt,
        up_block_types=ubt,
        attention_head_dim=args.attention_head_dim,
        timesteps=args.timesteps,
        eta_a=args.eta_a,
        eta_b=args.eta_b,
        eta_c=args.eta_c,
        sigma0=args.sigma0,
        num_workers=args.num_workers,
        seed=args.seed,
        mask_suffix=args.mask_suffix,
        mask_bin_thresh=args.mask_bin_thresh,
        known_is_white=args.known_is_white,
        skip_missing=True,
        save_y0=not args.no_save_y0,
        save_orig=not args.no_save_orig,
        save_last_only=not args.save_all_steps,
        step_interval=args.step_interval,
        num_diffusion_timesteps=args.num_diffusion_timesteps,
        beta_schedule=args.beta_schedule,
        prediction_type=args.prediction_type,
        masked_noise_init=True,
        init_noise_std=args.init_noise_std,
        mask_dilate_px=args.mask_dilate_px,
        do_refine=args.do_refine,
        refine_iters=args.refine_iters,
        refine_lr=args.refine_lr,
        w_lfreq=args.w_lfreq,
        w_hist=args.w_hist,
        w_tv=args.w_tv,
        w_seam=args.w_seam,
        seam_width=args.seam_width,
        optuna_mode=bool(args.optuna),
        optuna_trials=int(args.optuna_trials),
        optuna_samples=int(args.optuna_samples),
        optuna_batch_size=int(args.optuna_batch_size),
        optuna_n_jobs=int(args.optuna_n_jobs),
        optuna_aggressive_memory=bool(args.optuna_aggressive_memory),
        use_patch_inference=args.use_patch_inference,
        patch_size=args.patch_size,
        patch_stride=args.patch_stride,
    )

    # timesteps-list 파싱
    ts_list: List[int] = []
    # If Optuna has set a single best 'timesteps' value, use it
    if hasattr(args, '_optuna_timesteps_set') and args._optuna_timesteps_set:
        ts_list = [args.timesteps]
    else:
        for t in str(args.timesteps_list).split(","):
            t = t.strip()
            if not t:
                continue
            try:
                ts_list.append(int(t))
            except:
                pass
        if not ts_list:
            ts_list = [cfg.timesteps] # This cfg.timesteps is from args.timesteps
    cfg._timesteps_list = tuple(ts_list)  # 내부 사용 필드
    return cfg


def main():
    parser = build_argparser()
    args = parser.parse_args()

    # --- Auto-apply best Optuna params ---
    best_params_file = Path(args.output_dir) / "optuna_best_params.txt"
    if best_params_file.exists():
        print(f"[Info] Applying best parameters from {best_params_file}")
        with open(best_params_file, "r") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("best_value="):
                    continue
                key, value_str = line.split("=", 1)

                # Get the type from InpaintConfig for correct conversion
                # Use getattr to safely get annotation, default to str if not found
                param_type = InpaintConfig.__annotations__.get(key, str)

                if param_type == int:
                    setattr(args, key, int(value_str))
                elif param_type == float:
                    setattr(args, key, float(value_str))
                elif param_type == bool:
                    setattr(args, key, value_str.lower() == 'true')
                else: # Default to string for other types
                    setattr(args, key, value_str)

                if key == "timesteps":
                    setattr(args, '_optuna_timesteps_set', True)
    # --- End Auto-apply ---

    cfg = args_to_config(args)
    print("Config:", {k: v for k, v in asdict(cfg).items()
                     if k in ["images_dir","masks_dir","output_dir","ckpt_path",
                              "init_noise_std","invert_mask","mask_dilate_px",
                              "do_refine","refine_iters","w_lfreq","w_hist","w_tv","w_seam",
                              "optuna_mode","optuna_trials","optuna_samples"]})

    if cfg.optuna_mode:
        run_optuna(cfg)
    else:
        run_ddrm_inpaint(cfg)


if __name__ == "__main__":
    main()
